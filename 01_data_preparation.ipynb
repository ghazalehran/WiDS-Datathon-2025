{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# STEP 0: Download and access\n",
        "\n",
        "#colab + drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Download from Kaggle\n",
        "from google.colab import files\n",
        "files.upload()  # Upload the kaggle.json here\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c widsdatathon2025\n",
        "!unzip -q widsdatathon2025.zip -d /content/drive/MyDrive/WiDS/wids_data\n"
      ],
      "metadata": {
        "id": "9e2YU0vH0vha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-8TStffm0kAK"
      },
      "outputs": [],
      "source": [
        "# üì¶ STEP 1: IMPORTS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÅ STEP 2: LOAD RAW DATA\n",
        "base_path_data = \"/content/drive/MyDrive/WiDS/wids_data\"\n",
        "base_path_dir = \"/content/drive/MyDrive/WiDS Datathon\""
      ],
      "metadata": {
        "id": "JPWGxtoe4Rv_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load individual files\n",
        "labels_path = base_path_data + \"/TRAIN_NEW/TRAINING_SOLUTIONS.xlsx\"\n",
        "cat_path = base_path_data + \"/TRAIN_NEW/TRAIN_CATEGORICAL_METADATA_new.xlsx\"\n",
        "quant_path = base_path_data + \"/TRAIN_NEW/TRAIN_QUANTITATIVE_METADATA_new.xlsx\"\n",
        "connectome_path = base_path_data + \"/TRAIN_NEW/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\""
      ],
      "metadata": {
        "id": "FDWegngx4q5Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read files\n",
        "labels = pd.read_excel(labels_path)\n",
        "df_cat = pd.read_excel(cat_path)\n",
        "df_quant = pd.read_excel(quant_path)\n",
        "df_connectome = pd.read_csv(connectome_path)"
      ],
      "metadata": {
        "id": "4Kw-vRkm5Ke-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîó STEP 3: MERGE\n",
        "# Align everything by participant_id\n",
        "full = labels.merge(df_cat, on='participant_id')\n",
        "full = full.merge(df_quant, on='participant_id')\n",
        "full = full.merge(df_connectome, on='participant_id')"
      ],
      "metadata": {
        "id": "VtIXMXWT5SRY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† STEP 4: FEATURE SPLIT\n",
        "brain_columns = [col for col in full.columns if 'throw' in col]\n",
        "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
        "non_brain_columns = [col for col in full.columns if col not in brain_columns + target_cols + ['participant_id']]\n",
        "\n",
        "X_brain = full[brain_columns]\n",
        "X_non_brain = full[non_brain_columns]\n",
        "y = full[target_cols]"
      ],
      "metadata": {
        "id": "kToBAu2D5VwQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è STEP 5: PREPROCESSING PIPELINE\n",
        "numerical_cols = X_non_brain.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = X_non_brain.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', SimpleImputer(strategy='median'), numerical_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Scale brain features\n",
        "brain_scaler = StandardScaler()\n",
        "X_brain_scaled = brain_scaler.fit_transform(X_brain)\n",
        "\n",
        "# Process non-brain\n",
        "X_non_brain_processed = preprocessor.fit_transform(X_non_brain)"
      ],
      "metadata": {
        "id": "tpbrkB9Q5ewB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ STEP 6: SAVE ASSETS\n",
        "# Save X, y, and fitted transformers\n",
        "np.save(base_path_dir + \"/X_brain_scaled.npy\", X_brain_scaled)\n",
        "np.save(base_path_dir + \"/X_non_brain_processed.npy\", X_non_brain_processed)\n",
        "np.save(base_path_dir + \"/y.npy\", y.to_numpy())\n",
        "\n",
        "joblib.dump(preprocessor, base_path_dir + \"/final_preprocessor.pkl\")\n",
        "joblib.dump(brain_scaler, base_path_dir + \"/brain_scaler.pkl\")\n",
        "\n",
        "print(\"‚úÖ Data preparation complete and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQUulXk_5hzx",
        "outputId": "4503025b-e42f-4dea-e81d-e9bb3002468b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data preparation complete and saved.\n"
          ]
        }
      ]
    }
  ]
}